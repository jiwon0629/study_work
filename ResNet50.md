# 🧠 ResNet50: Deep Residual Learning 가이드
> **50개의 계층으로 구성된 심층 컨볼루션 신경망(CNN)의 혁신적 아키텍처 분석**

![ResNet50](https://img.shields.io/badge/ResNet--50-blue?style=for-the-badge)
![Deep Learning](https://img.shields.io/badge/Deep%20Learning-FF6F00?style=for-the-badge&logo=tensorflow&logoColor=white)

---

## 📌 1. ResNet50이란?
ResNet50은 50층으로 구성된 심층 컨볼루션 신경망입니다. 네트워크가 깊어질수록 학습이 어려워지는 **기울기 소실(Vanishing Gradient)** 문제를 처리하기 위해 **잔차 학습(Residual Learning)** 개념을 도입했습니다.

### ❓ 왜 이런 네트워크를 사용하는가?
* **문제점**: 역전파 과정에서 기울기가 점차 작아져 학습이 제대로 되지 않는 문제가 발생합니다.
* **해결책 (지름길 연결)**: 값을 출력 값에 직접 전달하는 **건너뛰기 연결(Skip Connection)**을 통해 기울기가 직접 흐를 수 있는 노선을 제공합니다.
* **공식**: $y = F(x) + x$

---

## 🏗️ 2. 주요 구조 및 레이어 역할

| 레이어 유형 | 주요 목적 | 특징 |
| :--- | :--- | :--- |
| **입력 레이어** | 이미지 입력 수용 | 표준 224×224×3 (RGB) 크기에 적합 |
| **컨볼루션** | 특징 추출 | 이미지의 방향, 그림자, 패턴 감지 (7x7, 3x3 사용) |
| **ReLU** | 비선형성 도입 | $f(x) = max(0, x)$ 공식을 통해 복잡한 패턴 학습 |
| **Batch Norm** | 학습 안정화 | 가중치 초기화 및 학습률 민감도를 줄여 안정적 학습 지원 |
| **Pooling** | 차원 축소 | Max Pooling(특징 설명) 및 Global Average Pooling(정보 압축) |

---

## 🧱 3. 핵심 혁신 기술

### 1️⃣ 잔차 블록 (Residual Block)
ResNet50의 중추를 형성하는 단위입니다.
* **ID 블록 (Identity Block)**: 입력과 출력의 크기가 같을 때 사용하며, 기능을 다듬는 데 집중합니다.
* **컨볼루션 블록**: 차원이 다를 때 사용하며, 지름길 연결 시 입력을 출력 치수에 맞게 투영합니다.

### 2️⃣ 병목 현상 (Bottleneck) 디자인
층의 계산 효율성을 높이기 위해 세 단계의 컨볼루션을 사용합니다.
* **1x1 Conv**: 채널 수를 줄여 차원을 축소합니다.
* **3x3 Conv**: 실제 공간적 특징을 추출합니다.
* **1x1 Conv**: 원래 차원으로 복원합니다.

---

## 📊 4. 최종 아키텍처 분석 (전체 50층)
ResNet50은 다음과 같은 레이어 그룹을 사용합니다:

* **Layer 1**: 1개의 Conv 블록 + 2개의 ID 블록
* **Layer 2**: 1개의 Conv 블록 + 3개의 ID 블록
* **Layer 3**: 1개의 Conv 블록 + 5개의 ID 블록
* **Layer 4**: 1개의 Conv 블록 + 2개의 ID 블록


---

## 🚀 5. 장점 및 응용
* **잔차 연결**: 성능 저하 없이 수백 개의 레이어로 네트워크 학습 가능
* **효율적인 추출**: 약 2,300만 개의 매개변수로 정확하면서도 계산 효율적임
* **응용 분야**: 이미지 분류, 객체 탐지(Faster R-CNN), 전이 학습 등

---
**Last Updated:** 2026-02-17
**Maintainer:** jiwon0629

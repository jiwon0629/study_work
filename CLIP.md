# 🚀 CLIP (Contrastive Language–Image Pre-training) 요약

본 문서는 OpenAI에서 발표한 **CLIP** 논문 및 기술 블로그 내용을 바탕으로 정리되었습니다. CLIP은 자연어 지도를 통해 시각적 개념을 효율적으로 학습하여 기존 컴퓨터 비전 모델의 주요 문제점들을 해결합니다.

---

## 1. 🌟 핵심 개념 및 주요 변화
* **자연어 지도 학습**: 인터넷에 공개된 방대한 이미지-텍스트 쌍 데이터를 활용하여 시각적 개념을 학습합니다.
* **제로샷(Zero-shot) 전이**: 특정 벤치마크 데이터에 직접 최적화하지 않고도, 자연어 지시만으로 다양한 분류 작업을 수행할 수 있습니다.
* **견고성(Robustness) 격차 해소**: ImageNet 외의 다양한 환경(추상적 표현, 다양한 배경 등)에서 발생하는 성능 저하를 최대 75%까지 줄였습니다.



---

## 2. 🛠️ 접근 방식 (Approach)
CLIP은 이미지와 텍스트를 하나의 공유된 공간(Embedding Space)으로 연결합니다.

1. **인코더 구성**: 이미지 인코더와 텍스트 인코더를 각각 사전 학습시킵니다.
2. **대조 학습(Contrastive Learning)**: 주어진 이미지에 대해 수만 개의 텍스트 중 실제 짝을 이룬 텍스트를 예측하도록 학습하여 시각적 개념과 이름을 연결합니다.
3. **분류기 생성**: 새로운 작업 시, 클래스 이름을 "개 사진"과 같은 캡션으로 변환하여 텍스트 인코더에 입력하면 즉석에서 선형 분류기가 생성됩니다.

---

## 3. 💡 기존 딥러닝 방식의 문제점 완화
* **비용 절감**: 수동 레이블링 대신 공개된 데이터를 활용하여 대규모 데이터셋 구축 비용을 낮췄습니다.
* **범용성 확대**: 특정 카테고리에 갇히지 않고, 추가 학습 없이 OCR, 동작 인식 등 30개 이상의 다양한 데이터셋에 적용 가능합니다.
* **실제 성능 반영**: 벤치마크 최적화(기출문제 공부)에 집중하지 않으므로 실제 배포 환경에서의 성능을 더 잘 반영합니다.

---

## 4. 📈 알고리즘 효율성
* **대조적 목적 함수**: 기존 방식보다 제로샷 분류에서 4배에서 10배 더 효율적인 학습이 가능합니다.
* **Vision Transformer (ViT) 채택**: 표준 ResNet 대비 연산 효율을 3배 더 향상시켰습니다.
* **성능 실적**: 최고의 CLIP 모델은 256개의 GPU에서 2주 동안 학습되어 최첨단 성능을 달성했습니다.

---

## ⚠️ 제한 사항 및 영향
* **복잡한 추상적 작업**: 사물 개수 세기나 거리 예측 등 복잡한 작업에서는 성능이 낮습니다.
* **미학습 데이터**: MNIST 손글씨처럼 학습 데이터에 포함되지 않은 특수 영역은 일반화가 어렵습니다.
* **사회적 편향**: 레이블 구성 방식에 따라 인종이나 성별에 대한 모델 편향이 나타날 수 있어 주의가 필요합니다.

---

## 🏁 결론
CLIP은 NLP의 작업 중립적 사전 학습 기법을 컴퓨터 비전에 성공적으로 이식했습니다. 이는 향후 모델의 기능과 한계, 편향 특성을 규명하는 연구의 중요한 전환점이 될 것입니다.

---
### 🔗 관련 링크
- [OpenAI CLIP 공식 블로그](https://openai.com/index/clip/)
- [Paper: Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)
